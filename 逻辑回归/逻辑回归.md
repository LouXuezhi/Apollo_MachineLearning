# 逻辑回归



主要用于分类任务

sigama 函数
$$
\sigma(x)=\frac{1}{1+e^{-x}}
$$
值域在0到1->压缩

输入项需要先进行线性变化：


$$
z=wx+b
$$
w 描述因子对分类结果的影响大小， b为偏置值



实际输出为 
$$
y （要么是0要么是1）
$$
预测输出为
$$
\hat{y}=\sigma(wx+b)
(输出一个介于0和1之间的数)
$$
线性回归中 损失函数为 预测值和实际值之间的平方差：

但是在逻辑回归中运用这种方法 则损失函数可能会变成非凸函数：

后果是在梯度下降的时候可能会陷入局部最小值而非全局最小值点


$$
y=1 \hspace{1cm} L=-\log(\hat{y})
\\
y=0 \hspace{1cm} L=- \log(1-\hat{y})
$$
合并两者得到 **二元交叉熵损失函数** 又叫**对数损失函数**
$$
L(y,\hat{y})=-[y\log(\hat{y})+(1-y)\log(1-\hat{y})]
$$
作为整个数据集，损失函数求平均变为**成本函数**
$$
J（w,b)=-\frac{1}{m} \sum_{i=1}^m \left[ y^{(i)}\log(\hat{y}^{(i)})+(1-y^{(i)})\log(1-\hat{y}^{(i)}) \right]
$$


梯度下降法：


$$
w_{new}=w_{old}-\alpha \frac{\partial J}{\partial w}
\\
b_{new }=b_{old}-\alpha\frac{\partial J}{\partial b}
$$






多元拓展：
$$
z=w_1x_1+w_2x_2 +... +w_nx_n +b
\\
z=\begin{bmatrix}w_1\\w_2\\.\\.\\w_n\end{bmatrix}^T\begin{bmatrix}x_1\\x_2\\.\\.\\x_n\end{bmatrix}+b\\
z=\mathbf{W}^T\mathbf{X}+b
$$
